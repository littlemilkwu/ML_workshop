{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input, regularizers\n",
    "from tensorflow.keras.backend import square, abs, sqrt\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "TRAIN_CSV_PATH = 'fake-news-pair-classification-challenge/train.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  tid1  tid2                          title1_zh  \\\n",
       "0   0     0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1   3     2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   1     2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "\n",
       "                   title2_zh  \\\n",
       "0   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "1  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "2       GDP首超香港？深圳澄清：还差一点点……   \n",
       "\n",
       "                                           title1_en  \\\n",
       "0  There are two new old-age insurance benefits f...   \n",
       "1  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2  \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                           title2_en      label  \n",
       "0  Police disprove \"bird's nest congress each per...  unrelated  \n",
       "1  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "2  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  "
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "cols = ['title1_zh', 'title2_zh', 'label']\n",
    "train = train.loc[:, cols]\n",
    "train.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title1_zh                  title2_zh      label\n",
       "0      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "1  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "2  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Segmetation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = \"我是小牛奶，是東吳巨資大四生\"\n",
    "words = pseg.cut(text)\n",
    "[word for word in words]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/fk/0dy994z53xdf6qfz76jn986c0000gn/T/jieba.cache\n",
      "Loading model cost 0.520 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[pair('我', 'r'),\n",
       " pair('是', 'v'),\n",
       " pair('小', 'a'),\n",
       " pair('牛奶', 'n'),\n",
       " pair('，', 'x'),\n",
       " pair('是', 'v'),\n",
       " pair('東吳', 'ns'),\n",
       " pair('巨資', 'n'),\n",
       " pair('大四', 'm'),\n",
       " pair('生', 'vn')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return \" \".join([word for word, flag in words if flag != 'x'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train['title1_tokenized'] = train.loc[:, 'title1_zh'].astype('str').apply(jieba_tokenizer)\n",
    "train['title2_tokenized'] = train.loc[:, 'title2_zh'].astype('str').apply(jieba_tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train.iloc[:, [0, 3]].head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title1_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title1_zh  \\\n",
       "0      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "1  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "\n",
       "                                  title1_tokenized  \n",
       "0         2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗  \n",
       "1  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "2  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train.iloc[:, [1, 4]].head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title2_zh                    title2_tokenized\n",
       "0   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
       "1  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
       "2       GDP首超香港？深圳澄清：还差一点点……            GDP 首 超 香港 深圳 澄清 还 差 一点点"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "corpus1 = train['title1_tokenized']\n",
    "corpus2 = train['title2_tokenized']\n",
    "corpus = pd.concat([corpus1, corpus2], axis=0)\n",
    "corpus.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(641104,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer.fit_on_texts(corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x1_train = tokenizer.texts_to_sequences(corpus1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus2)\n",
    "len(x1_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "320552"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "x1_train[:1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print([tokenizer.index_word[idx] for idx in x1_train[0]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## zero padding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for seq in x1_train[:10]:\n",
    "    print(len(seq), seq[:5], '...')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14 [217, 1268, 32, 1178, 5967] ...\n",
      "19 [4, 10, 47, 678, 2558] ...\n",
      "19 [4, 10, 47, 678, 2558] ...\n",
      "19 [4, 10, 47, 678, 2558] ...\n",
      "9 [31, 320, 3372, 3062, 1] ...\n",
      "19 [4, 10, 47, 678, 2558] ...\n",
      "6 [7, 2221, 1, 2072, 7] ...\n",
      "19 [4, 10, 47, 678, 2558] ...\n",
      "14 [1281, 1211, 427, 3, 3245] ...\n",
      "9 [31, 320, 3372, 3062, 1] ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "max_seq_len = max([len(seq) for seq in x1_train])\n",
    "max_seq_len"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "MAX_SEQ_LEN = 30\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, maxlen=MAX_SEQ_LEN)\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, maxlen=MAX_SEQ_LEN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 30\n",
    "print('all seqs are len 30')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "all seqs are len 30\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "x1_train[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,  217, 1268,   32, 1178, 5967,   25,\n",
       "         489, 2877,  116, 5559,    4, 1850,    2,   13],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           4,   10,   47,  678, 2558,    4,  166,   34,   17,   47, 5150,\n",
       "          63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           4,   10,   47,  678, 2558,    4,  166,   34,   17,   47, 5150,\n",
       "          63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           4,   10,   47,  678, 2558,    4,  166,   34,   17,   47, 5150,\n",
       "          63,   15,  678, 4502, 3211,   23,  284, 1181],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   31,\n",
       "         320, 3372, 3062,    1,   95,   98, 3372, 3062]], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "train['label'][:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    unrelated\n",
       "1    unrelated\n",
       "2    unrelated\n",
       "3    unrelated\n",
       "4       agreed\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "label_to_index = {\n",
    "    'unrelated': 0,\n",
    "    'agreed': 1, \n",
    "    'disagreed': 2,\n",
    "}\n",
    "y_train = train['label'].apply(lambda x: label_to_index[x])\n",
    "y_train = y_train.astype('float32').to_numpy()\n",
    "y_train[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_train[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "VALIDATION_RATION = 0.2\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(x1_train, x2_train, y_train, test_size=VALIDATION_RATION, random_state=RANDOM_STATE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (256441, 30)\n",
      "x2_train: (256441, 30)\n",
      "y_train : (256441, 3)\n",
      "----------\n",
      "x1_val:   (64111, 30)\n",
      "x2_val:   (64111, 30)\n",
      "y_val :   (64111, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# lstm = layers.LSTM()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "NUM_CLASSES = 3\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "MAX_SEQ_LEN = 30\n",
    "\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "NUM_LSTM_UNITS = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "## normal\n",
    "# top_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "# bm_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "\n",
    "# embedding_layer = layers.Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "# top_embedding = embedding_layer(top_input)\n",
    "# bm_embedding = embedding_layer(bm_input)\n",
    "\n",
    "# shared_LSTM = layers.LSTM(NUM_LSTM_UNITS)\n",
    "# top_output = shared_LSTM(top_embedding)\n",
    "# bm_output = shared_LSTM(bm_embedding)\n",
    "\n",
    "# merged = layers.concatenate([top_output, bm_output], axis=-1)\n",
    "\n",
    "# dense = layers.Dense(units=NUM_CLASSES, activation='softmax')\n",
    "# predictions = dense(merged)\n",
    "\n",
    "# model = Model(inputs=[top_input, bm_input], outputs=predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "## add\n",
    "# top_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "# bm_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "\n",
    "# embedding_layer = layers.Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "# top_embedding = embedding_layer(top_input)\n",
    "# bm_embedding = embedding_layer(bm_input)\n",
    "\n",
    "# shared_LSTM = layers.LSTM(NUM_LSTM_UNITS)\n",
    "# top_output = shared_LSTM(top_embedding)\n",
    "# bm_output = shared_LSTM(bm_embedding)\n",
    "\n",
    "# added = layers.Add()([top_output, bm_output])\n",
    "# # merged = layers.concatenate([top_output, bm_output], axis=-1)\n",
    "\n",
    "# dense = layers.Dense(units=NUM_CLASSES, activation='softmax')\n",
    "# predictions = dense(added)\n",
    "\n",
    "# model = Model(inputs=[top_input, bm_input], outputs=predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def Euclidean_distance(a, b):\n",
    "    return square(a - b)\n",
    "\n",
    "top_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "bm_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "\n",
    "embedding_layer = layers.Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedding = embedding_layer(top_input)\n",
    "bm_embedding = embedding_layer(bm_input)\n",
    "\n",
    "shared_LSTM = layers.LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_LSTM(top_embedding)\n",
    "bm_output = shared_LSTM(bm_embedding)\n",
    "\n",
    "merged = layers.Lambda(\n",
    "    lambda x: Euclidean_distance(x[0], x[1]), # x is list of input shape\n",
    ")([top_output, bm_output])\n",
    "\n",
    "dense = layers.Dense(units=NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(l=0.001))\n",
    "predictions = dense(merged)\n",
    "\n",
    "model = Model(inputs=[top_input, bm_input], outputs=predictions)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# def abs_distance(a, b):\n",
    "#     return abs(a - b)\n",
    "\n",
    "# top_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "# bm_input = Input(shape=(MAX_SEQ_LEN, ), dtype=\"int32\")\n",
    "\n",
    "# embedding_layer = layers.Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "# top_embedding = embedding_layer(top_input)\n",
    "# bm_embedding = embedding_layer(bm_input)\n",
    "\n",
    "# shared_LSTM = layers.LSTM(NUM_LSTM_UNITS)\n",
    "# top_output = shared_LSTM(top_embedding)\n",
    "# bm_output = shared_LSTM(bm_embedding)\n",
    "\n",
    "# merged = layers.Lambda(\n",
    "#     lambda x: abs_distance(x[0], x[1]), # x is list of input shape\n",
    "# )([top_output, bm_output])\n",
    "\n",
    "# dense = layers.Dense(units=NUM_CLASSES, activation='softmax')\n",
    "# predictions = dense(merged)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 30, 256)      2560000     input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 128)          197120      embedding_8[0][0]                \n",
      "                                                                 embedding_8[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 128)          0           lstm_8[0][0]                     \n",
      "                                                                 lstm_8[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            387         lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,757,507\n",
      "Trainable params: 2,757,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "NUM_EPOCHS = 7\n",
    "\n",
    "history = model.fit(\n",
    "    x=[x1_train, x2_train], \n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    validation_data=(\n",
    "        [x1_val, x2_val], \n",
    "        y_val\n",
    "    ),\n",
    "    shuffle=True\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/7\n",
      "1002/1002 [==============================] - 197s 194ms/step - loss: 0.5453 - accuracy: 0.7543 - val_loss: 0.4363 - val_accuracy: 0.8062\n",
      "Epoch 2/7\n",
      "1002/1002 [==============================] - 201s 200ms/step - loss: 0.3815 - accuracy: 0.8376 - val_loss: 0.3720 - val_accuracy: 0.8407\n",
      "Epoch 3/7\n",
      "1002/1002 [==============================] - 203s 203ms/step - loss: 0.3172 - accuracy: 0.8700 - val_loss: 0.3505 - val_accuracy: 0.8517\n",
      "Epoch 4/7\n",
      "1002/1002 [==============================] - 210s 209ms/step - loss: 0.2746 - accuracy: 0.8914 - val_loss: 0.3355 - val_accuracy: 0.8619\n",
      "Epoch 5/7\n",
      "1002/1002 [==============================] - 208s 207ms/step - loss: 0.2413 - accuracy: 0.9070 - val_loss: 0.3313 - val_accuracy: 0.8683\n",
      "Epoch 6/7\n",
      "1002/1002 [==============================] - 213s 213ms/step - loss: 0.2135 - accuracy: 0.9194 - val_loss: 0.3317 - val_accuracy: 0.8691\n",
      "Epoch 7/7\n",
      "1002/1002 [==============================] - 211s 211ms/step - loss: 0.1901 - accuracy: 0.9304 - val_loss: 0.3352 - val_accuracy: 0.8710\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "TEST_CSV_PATH = \"fake-news-pair-classification-challenge/test.csv\"\n",
    "test = pd.read_csv(TEST_CSV_PATH, index_col=0)\n",
    "test.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321187</th>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321190</th>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321189</th>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tid1    tid2                        title1_zh  \\\n",
       "id                                                        \n",
       "321187  167562   59521  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321190  167564   91315              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321189  167563  167564    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "\n",
       "                          title2_zh  \\\n",
       "id                                    \n",
       "321187  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                                                title1_en  \\\n",
       "id                                                          \n",
       "321187  egypt 's presidential election failed to win m...   \n",
       "321190  A message from Saddam Hussein after he was cap...   \n",
       "321189  Will the United States wage war on Iraq withou...   \n",
       "\n",
       "                                                title2_en  \n",
       "id                                                         \n",
       "321187  Lyon! Lyon officials have denied that Felipe F...  \n",
       "321190  The Top 10 Americans believe that the Lizard M...  \n",
       "321189  A message from Saddam Hussein after he was cap...  "
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "test = test.iloc[:, [2, 3]]\n",
    "test.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321187</th>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321190</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321189</th>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title1_zh                    title2_zh\n",
       "id                                                                  \n",
       "321187  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？\n",
       "321190              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国\n",
       "321189    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "test['title1_tokenized'] = test['title1_zh'].astype('str').apply(jieba_tokenizer)\n",
    "test['title2_tokenized'] = test['title2_zh'].astype('str').apply(jieba_tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# tokenize\n",
    "x1_test = tokenizer.texts_to_sequences(test['title1_tokenized'])\n",
    "x2_test = tokenizer.texts_to_sequences(test['title2_tokenized'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# zero padding\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test, maxlen=MAX_SEQ_LEN)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test, maxlen=MAX_SEQ_LEN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "predictions = model.predict([x1_test, x2_test])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "predictions[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[9.9564725e-01, 9.7066441e-06, 4.3429662e-03],\n",
       "       [9.2297775e-01, 3.5294704e-04, 7.6669298e-02],\n",
       "       [9.1102898e-02, 9.0695900e-01, 1.9381513e-03],\n",
       "       [9.2342085e-01, 8.2459586e-04, 7.5754516e-02],\n",
       "       [9.6152264e-01, 7.7072706e-04, 3.7706628e-02],\n",
       "       [2.3397334e-01, 7.6446277e-01, 1.5638802e-03],\n",
       "       [9.9343687e-01, 3.4138761e-04, 6.2217303e-03],\n",
       "       [2.2440670e-01, 7.7466857e-01, 9.2467858e-04],\n",
       "       [7.6061058e-01, 2.2778060e-01, 1.1608930e-02],\n",
       "       [8.5276204e-01, 1.4637919e-01, 8.5875957e-04]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "\n",
    "submission = test.loc[:, ['Category']].reset_index()\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   Category\n",
       "0  321187  unrelated\n",
       "1  321190  unrelated\n",
       "2  321189     agreed"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ML_workshop': conda)"
  },
  "interpreter": {
   "hash": "aa156bad5c45b204a11d7d58485823703737cf70d6ca4252563a7670c480f6d3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}